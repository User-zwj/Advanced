{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/learning-model-building-scikit-learn-python-machine-learning-library/\n",
    "\n",
    "pip install -U scikit-learn\n",
    "\n",
    "# Step 1: Load a dataset\n",
    "**Loading exemplar dataset:** scikit-learn comes loaded with a few example datasets like the iris and digits datasets for classification and the boston house prices dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset as an example \n",
    "from sklearn.datasets import load_iris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Type of X is: <class 'numpy.ndarray'>\n",
      "\n",
      "First 5 rows of X:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "First 5 rows of y:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris() \n",
    "# print(iris)\n",
    "\n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "# store the feature and target names \n",
    "feature_names = iris.feature_names \n",
    "target_names = iris.target_names \n",
    "\n",
    "# printing features and target names of our dataset \n",
    "print(\"Feature names:\", feature_names) \n",
    "print(\"Target names:\", target_names) \n",
    "\n",
    "# X and y are numpy arrays \n",
    "print(\"\\nType of X is:\", type(X)) \n",
    "\n",
    "# printing first 5 input rows \n",
    "print(\"\\nFirst 5 rows of X:\\n\", X[:5])\n",
    "\n",
    "print(\"\\nFirst 5 rows of y:\\n\", y[30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa\n",
      "versicolor\n",
      "virginica\n"
     ]
    }
   ],
   "source": [
    "print(iris.target_names[0])\n",
    "print(iris.target_names[1])\n",
    "print(iris.target_names[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading external dataset:** Now, consider the case when we want to load an external dataset. For this purpose, we can use pandas library for easily loading and manipulating dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (14, 5)\n",
      "\n",
      "Features: Index(['Outlook', 'Temperature', 'Humidity', 'Windy', 'Play'], dtype='object')\n",
      "\n",
      "Feature matrix:\n",
      "     Outlook Temperature Humidity  Windy\n",
      "0  overcase         hot     high  False\n",
      "1  overcase        cool   normal   True\n",
      "2  overcase        mild     high   True\n",
      "3  overcase         hot   normal  False\n",
      "4     rainy        mild     high  False\n",
      "\n",
      "Response vector:\n",
      " 0    yes\n",
      "1    yes\n",
      "2    yes\n",
      "3    yes\n",
      "4    yes\n",
      "Name: Play, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# reading csv file \n",
    "data = pd.read_csv('weather.csv') \n",
    "\n",
    "# shape of dataset \n",
    "print(\"Shape:\", data.shape) \n",
    "\n",
    "# column names \n",
    "print(\"\\nFeatures:\", data.columns) \n",
    "\n",
    "# storing the feature matrix (X) and response vector (y) \n",
    "X = data[data.columns[:-1]] \n",
    "y = data[data.columns[-1]] \n",
    "\n",
    "# printing first 5 rows of feature matrix \n",
    "print(\"\\nFeature matrix:\\n\", X.head()) \n",
    "\n",
    "# printing first 5 values of response vector \n",
    "print(\"\\nResponse vector:\\n\", y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset as an example \n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "(60, 4)\n",
      "(90,)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris() \n",
    "\n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "# splitting X and y into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "\n",
    "# printing the shapes of the new X objects \n",
    "print(X_train.shape) \n",
    "print(X_test.shape) \n",
    "\n",
    "# printing the shapes of the new y objects \n",
    "print(y_train.shape) \n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn import metrics \n",
    "from sklearn.externals import joblib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 2]\n",
      "kNN model accuracy: 0.9833333333333333\n",
      "Predictions: ['versicolor', 'virginica']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['iris_knn.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the iris dataset as an example \n",
    "iris = load_iris() \n",
    "\n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "# splitting X and y into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "\n",
    "# training the model on training set \n",
    "knn = KNeighborsClassifier(n_neighbors=3) \n",
    "knn.fit(X_train, y_train) \n",
    "\n",
    "# making predictions on the testing set \n",
    "y_pred = knn.predict(X_test) \n",
    "print(y_pred[:5])\n",
    "\n",
    "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
    "print(\"kNN model accuracy:\", metrics.accuracy_score(y_test, y_pred)) \n",
    "\n",
    "# making prediction for out of sample data \n",
    "sample = [[3, 5, 4, 2], [2, 3, 5, 4]] \n",
    "preds = knn.predict(sample) \n",
    "pred_species = [iris.target_names[p] for p in preds] \n",
    "print(\"Predictions:\", pred_species) \n",
    "\n",
    "# saving the model \n",
    "joblib.dump(knn, 'iris_knn.pkl') \n",
    "# If you are not interested in training your classifier again and again and use the pre-trained classifier, one can save their classifier using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to load an already saved classifier, use the following method:\n",
    "knn = joblib.load('iris_knn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
